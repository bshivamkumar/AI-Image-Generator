<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building an Accessible AI Image Generator</title>
    <!-- Load Tailwind CSS from CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Use Inter font family -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* Light gray background */
        }
        .report-container {
            max-width: 800px;
            margin: 40px auto;
            padding: 30px;
            background: white;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            border-radius: 12px;
        }
        h1 {
            border-bottom: 3px solid #6366f1; /* Indigo border for header */
            padding-bottom: 8px;
            margin-bottom: 24px;
        }
        h2 {
            margin-top: 30px;
            margin-bottom: 15px;
            color: #1e3a8a; /* Dark blue for section titles */
            font-weight: 700;
        }
        h3 {
            color: #4f46e5; /* Violet for subsections */
            font-weight: 600;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.75;
            color: #334155;
            margin-bottom: 12px;
        }
        .team-info {
            color: #64748b;
            font-style: italic;
            margin-bottom: 24px;
        }
        .note {
            background-color: #fef3c7;
            border-left: 4px solid #f59e0b;
            padding: 15px;
            border-radius: 4px;
            margin-top: 20px;
            font-size: 0.9em;
            color: #92400e;
        }
        .table-container {
            overflow-x: auto;
            margin-top: 20px;
            margin-bottom: 20px;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.95em;
        }
        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #e2e8f0;
        }
        th {
            background-color: #eef2ff; /* Light indigo background */
            color: #3730a3;
            font-weight: 600;
        }
        tr:hover {
            background-color: #f9fafb;
        }
        .image-placeholder {
            background-color: #e0e7ff;
            color: #4338ca;
            padding: 30px;
            margin: 20px 0;
            border-radius: 8px;
            text-align: center;
            border: 2px dashed #818cf8;
            font-weight: 600;
        }
    </style>
</head>
<body>

<div class="report-container">

    <h1 class="text-4xl font-extrabold text-gray-900">Building an Accessible AI Image Generator for Everyone</h1>

    <div class="team-info">
        <p><strong>Team:</strong> Shouhan Wang, Bhardwaj Shivam, Jingong Chen</p>
        <p><strong>Project:</strong> AI-Powered Image Generation Model (Text to Image)</p>
    </div>
    
    <hr class="mt-8 mb-6 border-gray-200">

    <!-- 1. Problem Definition -->
    <h2 class="text-2xl">1. Problem Definition </h2>
    <p>The generative AI landscape is rapidly evolving, yet barriers to entry—chiefly <strong>high operational costs and massive computing resource requirements</strong>—prevent individual creators and small businesses from leveraging these tools fully. Current state-of-the-art models, while powerful, often demand specialized hardware or expensive API subscriptions, contributing to the rising labor costs associated with content creation. We observed a crucial gap: the need for a solution that democratizes high-quality visual content generation.</p>
    <p>Our solution, the <strong>AI-Powered Image Generation Model</strong>, addresses this by prioritizing efficiency and accessibility. We set out to develop a system that, whether through direct deployment or by enabling better fine-tuning data collection, significantly lowers the computational and financial burden of image creation. By focusing on a *lightweight* approach, we aim to provide an efficient alternative to established heavyweights like DALL-E 3 and Midjourney, enabling users—from property managers needing quick floor plans to educators needing visual aids—to generate customized imagery without requiring large-scale computing infrastructure. The ultimate goal is to move the industry toward more resource-aware, user-friendly AI deployment.</p>

    <!-- 2. System Design -->
    <h2 class="text-2xl">2. System Design </h2>
	<div class="my-8 space-y-6">
        <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
            <img src="system_design.png" alt="Detailed System Architecture Diagram" class="w-full h-auto rounded-lg">
            <p class="text-center text-sm mt-2 text-gray-500">Figure 1.1: Overall System Pipeline highlighting key components (Frontend, Inference, Data Loop).</p>
        </div>
    <p>Our system is engineered as a robust, full-stack application centered around providing a rapid, interactive image generation and refinement experience.</p>

    <h3>Key Components</h3>
    <ol class="list-decimal ml-6">
        <li><strong>Frontend (Client-Side):</strong> Developed using <strong>React.js</strong> and styled with <strong>Tailwind CSS</strong>, the frontend is a single-page application (SPA) designed for responsiveness and speed. Its primary functions are:
            <ul class="list-disc ml-6 mt-2">
                <li><strong>Prompt Interface:</strong> Accepts user text input for generation.</li>
                <li><strong>Generation & Display:</strong> Manages the API request lifecycle and displays the resulting images in a unique <strong>four-quadrant grid</strong>.</li>
                <li><strong>Performance Monitoring:</strong> Measures and visualizes the generation time (latency) for each request, offering users transparent performance metrics.</li>
                <li><strong>Feedback Mechanism:</strong> Implements a direct, per-quadrant feedback modal to capture explicit user critique on specific image areas.</li>
            </ul>
        </li>
        <li><strong>Backend/Inference Service:</strong> For the Minimum Viable Product (MVP), we utilized the <strong>Hugging Face Inference API</strong> to host the <strong>Stable Diffusion XL (SDXL) 1.0</strong> model. This choice allowed us to focus development efforts on the user experience and data collection pipeline while leveraging a highly optimized, high-quality foundational model. This architecture minimizes our immediate server maintenance overhead.</li>
        <li><strong>Data Pipeline (Feedback Loop):</strong> The core value of the application lies in its data collection layer. User-submitted prompts and the explicit textual feedback associated with specific generated quadrants are captured. This data is the basis for our future iterative development.</li>
    </ol>

    <h3>Design and Technology Justifications</h3>
    <p>We made several central design decisions to achieve our goal of accessibility and efficiency:</p>
    <ul class="list-disc ml-6">
        <li><strong>Technology Stack (React/SPA):</strong> A web application interface was chosen over an API-only interface because the <strong>visual nature of the problem</strong> demands immediate user interaction and feedback. A responsive web app allows users across various devices (mobile, desktop) to access the tool without any installation, which is critical for small businesses.</li>
        <li><strong>Four-Quadrant Display:</strong> Instead of generating one image, we generate four variations simultaneously. This design choice serves two purposes: first, it rapidly showcases the model's creative range; second, it provides four distinct samples for the user to evaluate, significantly accelerating the collection of high-quality, targeted feedback for a future lightweight model fine-tuning dataset.</li>
        <li><strong>Latency Visualization:</strong> The inclusion of a visible bar chart showing generation time validates our core thesis about efficiency and provides real-time transparency into system performance.</li>
        <li><strong>ML Model Selection:</strong> While our long-term goal is a fully custom, lightweight model, using SDXL 1.0 in the MVP provides a high-quality baseline. This allows us to collect data that can be used to fine-tune a smaller, faster model (e.g., a distilled version) with domain-specific knowledge, aligning with our resource-constrained target users.</li>
    </ul>
	<!-- System Design and Feedback Loop Diagrams -->
		<div class="my-8 space-y-6">
        <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
            <img src="demo2.png" alt="Text_to_Image Prompt Interface" class="w-full h-auto rounded-lg">
            <p class="text-center text-sm mt-2 text-gray-500">Figure 1.2:Text_to_Image Prompt Interface</p>
        </div>
        
        <div class="grid md:grid-cols-2 gap-6">
            <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
                <img src="feedback_submission_comment.png" alt="Screenshot of the feedback submission modal" class="w-full h-auto rounded-lg">
                <p class="text-center text-sm mt-2 text-gray-500">Figure 1.3: Feedback submission interface (modal) where users provide specific critique on an image quadrant.</p>
            </div>
            <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
                <img src="feedback_viewer.png" alt="Screenshot of the collected feedback data viewer" class="w-full h-auto rounded-lg">
                <p class="text-center text-sm mt-2 text-gray-500">Figure 1.4: Data viewer illustrating how structured feedback is recorded for model fine-tuning.</p>
            </div>
        </div>
    </div>

    <!-- 3. Machine Learning Component -->
    <h2 class="text-2xl">3. Machine Learning Component </h2>
    <p>The ML component of our application is not just the model itself, but the entire iterative process designed to collect better data for future model improvement.</p>

    <h3>The Core Model and Data</h3>
    <p>For the application's launch, we implemented the <strong>Stable Diffusion XL (SDXL) 1.0</strong> text-to-image model. This latent diffusion model represents a powerful foundational capability, allowing us to generate photorealistic images from text prompts. We integrated this model via the Hugging Face Inference API, abstracting away the heavy lifting of direct GPU management.</p>
    <p>While SDXL was trained on a massive, proprietary dataset of filtered text-image pairs (a general, broad corpus), our project’s focus shifted to collecting a *targeted* dataset.</p>

    <h3>The Iterative Development: Data Request Collection</h3>
    <p>Our primary objective was to move beyond passively accepting text prompts and to actively capture <strong>paired generated images and specific human feedback</strong>. This is where our iterative development shines:</p>
    <ol class="list-decimal ml-6">
        <li><strong>Generation:</strong> The user inputs a prompt (e.g., "A digital painting of a sun-drenched sunflower").</li>
        <li><strong>Variations:</strong> The system generates four distinct images in the quadrant grid.</li>
        <li><strong>Refinement Loop:</strong> The user selects a quadrant and provides targeted, textual feedback (e.g., "The sunflower looks too blurry here" or "The background color is perfect!").</li>
    </ol>
    <p>This process effectively creates a unique, high-value dataset for future iterations: <code>[Original Prompt, Generated Image (Quadrant X), Localized Textual Feedback]</code>. This data is crucial because generic image generation models often struggle with user-specific aesthetics and detail. By collecting *explicit* human preference data, we pave the way for fine-tuning a smaller model specifically for high-quality results within our target domains (e.g., stylized concepts, simple product visualization), enabling us to ultimately achieve the <strong>lightweight, efficient model</strong> initially proposed. The current application is, therefore, a sophisticated <strong>data collection and labeling tool</strong> disguised as a user-friendly image generator.</p>

    <!-- 4. System Evaluation -->
    <h2 class="text-2xl">4. System Evaluation </h2>
    <p>Evaluating our text-to-image system involved assessing both its objective performance (speed) and its subjective output (image quality and utility).</p>

    <h3>Performance Validation</h3>
    <p>We focused on <strong>latency</strong> as a core metric, aligning with our goal of efficiency. Using the built-in performance monitor, we tracked the time from prompt submission to the display of the four final images.</p>

    <div class="table-container">
        <table>
            <thead>
                <tr>
                    <th>Test Scenario</th>
                    <th>Average Latency (seconds)</th>
                    <th>Baseline Comparison</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Simple Prompt (e.g., "A cat")</td>
                    <td>8.5 - 12.0 s</td>
                    <td>Faster than complex AIs requiring manual staging/server setup.</td>
                </tr>
                <tr>
                    <td>Complex Prompt (e.g., "A detailed oil painting of a futuristic cityscape at sunset")</td>
                    <td>15.0 - 20.0 s</td>
                    <td>Competitive with commercial APIs running SDXL.</td>
                </tr>
            </tbody>
        </table>
    </div>
    <p>The latency visualization within the application provides immediate performance feedback, validating that the overall system pipeline—including the React component rendering and API call overhead—offers a responsive experience. This confirms our design choice to leverage a highly-optimized third-party inference endpoint for speed.</p>
    <!-- Performance Analysis Chart -->
    <div class="my-8">
        <img src="performance_analysis.png" alt="Chart showing performance analysis and latency metrics for image generation" class="w-full h-auto rounded-xl shadow-xl border border-indigo-200">
        <p class="text-center text-sm mt-2 text-gray-500">Figure 3: Visualization of system latency for various prompt complexities, validating efficiency goals.</p>
    </div>
    <h3>Limitations and Subjective Evaluation</h3>
    <p>The main limitations center on the MVP's reliance on the third-party API and the general constraints of foundational models:</p>
    <ul class="list-disc ml-6">
        <li><strong>API Dependence:</strong> Performance is tightly coupled with Hugging Face’s uptime and resource allocation, introducing an external point of failure.</li>
        <li><strong>Generalization vs. Specificity:</strong> While SDXL is excellent at general concepts, it often struggles with highly specific, non-mainstream aesthetic requests or generating consistent text within images. This highlights the need for the targeted fine-tuning data we are currently collecting.</li>
        <li><strong>Bias and Ethical Constraints:</strong> As noted in our proposal, foundational models can inherit biases from their training data. Our system needs continuous monitoring for outputs that are inappropriate, stereotypical, or violate copyright. The feedback loop is the first line of defense, allowing users to flag problematic results.</li>
    </ul>

    <h3>Utility Assessment</h3>
    <p>The four-quadrant design proved highly effective for utility. Users could quickly zero in on a desirable aesthetic and provide specific feedback on the image *most* aligned with their vision. This high-resolution feedback is the primary *result* of our system evaluation, as it proves the mechanism is viable for building a superior, lightweight, and domain-specific model in the future. The quality of the generated images, even without fine-tuning, was sufficient for mock-ups in educational and small business contexts.</p>

    <!-- 5. Application Demonstration -->
    <h2 class="text-2xl">5. Application Demonstration </h2>
    <p>The application provides a clean, intuitive interface designed for rapid iteration.</p>
    <!-- Application Demonstration Screenshots -->
    <div class="my-8 space-y-6">
        <div class="grid md:grid-cols-2 gap-6">
            <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
                <img src="demo1.png" alt="Main application view with input prompt and generated images" class="w-full h-auto rounded-lg">
                <p class="text-center text-sm mt-2 text-gray-500">Figure 4.1: Initial view of the generator interface (Prompt and Generation Grid).</p>
            </div>
            <div class="border border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
                <img src="generate.png" alt="Close-up of the generate button and input area" class="w-full h-auto rounded-lg">
                <p class="text-center text-sm mt-2 text-gray-500">Figure 4.2: Focus on the input/command area.</p>
            </div>
            <div class="border md:col-span-2 grid md:grid-cols-2 gap-6 border-indigo-200 rounded-xl shadow-xl p-4 bg-white">
                <div>
                    <img src="feedback1.png" alt="Screenshot showing the feedback button on an image quadrant" class="w-full h-auto rounded-lg">
                    <p class="text-center text-sm mt-2 text-gray-500">Figure 4.3: User interaction: triggering the feedback mechanism on a selected quadrant.</p>
                </div>
                <div>
                    <img src="feedback2.png" alt="Screenshot showing the final feedback confirmation" class="w-full h-auto rounded-lg">
                    <p class="text-center text-sm mt-2 text-gray-500">Figure 4.4: Confirmation of feedback submission, completing the data loop.</p>
                </div>
            </div>
        </div>
    </div>

    <h3>Core Feature Set</h3>
    <ol class="list-decimal ml-6">
        <li><strong>Prompt Entry:</strong> Users enter a descriptive text prompt into the input field.</li>
        <li><strong>Four-Image Generation:</strong> Upon clicking "Generate," four variations are returned and displayed simultaneously.</li>
        <li><strong>Performance Display:</strong> A small graph updates to show the generation time, reinforcing the focus on efficiency.</li>
        <li><strong>Targeted Feedback:</strong> Hovering over any quadrant reveals a 'Feedback' button. Clicking this opens a modal where users can type specific critiques about that exact image variant.</li>
    </ol>

    <h3>Interface Justification</h3>
    <p>The decision to use a <strong>Web Application interface</strong> was superior to a command-line utility or a pure API because of the project's focus on <strong>data collection and user experience</strong>. Visualizing the output immediately, providing a visual comparison grid, and integrating the feedback mechanism directly into the image display creates a seamless, low-friction experience necessary to gather high-quality, labeled data efficiently. It transforms a technical ML task into an accessible, collaborative design tool.</p>

    <h3>Instructions</h3>
    <ol class="list-decimal ml-6">
        <li>Enter a descriptive prompt into the text box (e.g., *“A watercolor painting of a red panda sitting on a moon, studio lighting”*).</li>
        <li>Click the <strong>Generate</strong> button.</li>
        <li>Wait for the four images to appear.</li>
        <li>If you like an image and want to help us refine our future model, hover over that image, click the <strong>Feedback</strong> button, and submit a brief, specific critique.</li>
    </ol>

    <!-- 6. Reflection -->
    <h2 class="text-2xl">6. Reflection </h2>

    <h3>What Worked?</h3>
    <p>Our team dynamics were a major success. By dividing the work into clear, isolated components (Frontend UX, ML Integration, and Data Pipeline Design), we achieved a fast development pace. The choice of <strong>React with Tailwind CSS</strong> for the frontend proved invaluable for quickly building a highly responsive and visually appealing interface. Critically, the pivot to leveraging a reliable third-party API for the MVP allowed us to deliver a fully functional, high-quality product that solved the <strong>user experience</strong> and <strong>data collection</strong> problem, even while the *model-training* piece was pending. The four-quadrant display and dedicated feedback loop were highly effective design decisions that garnered positive initial feedback.</p>

    <h3>What Didn’t Work? And What Would We Improve?</h3>
    <p>The biggest challenge was the time constraint on building and fine-tuning our original <strong>truly lightweight custom model</strong>. The shift to an SDXL-based MVP, while pragmatic, meant we didn't fully achieve the *resource-efficiency* goal in the first iteration.</p>
    <p>Next time, we would implement <strong>immediate, real-time data storage</strong> (e.g., using a non-SQL database like Firestore) for the feedback data, rather than relying solely on the temporary in-memory capture structure used in the initial demo. We would also add a more rigorous <strong>user authentication/session management</strong> to track feedback quality and prevent prompt abuse.</p>

    <h3>If Given Unlimited Time and Resources</h3>
    <p>With unlimited resources, we would focus on three main areas:</p>
    <ol class="list-decimal ml-6">
        <li><strong>Dedicated GPU Cluster and Custom Model Training:</strong> We would dedicate resources to training a highly efficient, custom-distilled text-to-image model using our collected feedback data. This would be a lightweight model specifically optimized for low-latency inference on commodity hardware, finally achieving our core efficiency goal.</li>
        <li><strong>Integrated Content Moderation:</strong> Implement a robust safety filtering layer, combining image-to-text analysis with pre-trained safety models to automatically flag and remove sensitive or copyrighted content, moving beyond simple policy enforcement.</li>
        <li><strong>Image-to-Image and Inpainting Features:</strong> Expand the feature set to allow users to upload their own images for editing or to refine specific areas of a generated image, leveraging the power of our data collection system for even more granular control.</li>
    </ol>

    <h3>Future Plans</h3>
    <p>Our immediate next steps are to move the prototype into a production data-gathering phase. We plan to <strong>open-source the frontend feedback component</strong> to encourage community contributions and potentially apply for a research grant to cover the initial costs of fine-tuning the first iteration of the truly lightweight model, using the high-quality, labeled feedback data we are now collecting. We are excited about the prospect of continuing this project and truly democratizing generative AI.</p>

    <!-- 7. Broader Impacts -->
    <h2 class="text-2xl">7. Broader Impacts </h2>

    <h3>Intended Uses</h3>
    <p>The primary intended uses for our application are benign and productivity-focused:</p>
    <ol class="list-decimal ml-6">
        <li><strong>Small Business Marketing:</strong> Rapid generation of concept art, website mockups, and social media visuals.</li>
        <li><strong>Education:</strong> Creating quick, customized visual aids for complex topics to enhance learning.</li>
        <li><strong>Property Management:</strong> Generating preliminary floor plans or spatial renderings to aid client communication.</li>
    </ol>

    <h3>Possible Unintended Uses and Harms</h3>
    <p>The most significant potential harms associated with any generative AI system are <strong>misinformation</strong> (the creation of convincing deepfakes or misleading imagery) and <strong>copyright infringement</strong> (generating images too closely resembling existing protected works). Additionally, the model, due to biases in its vast training data, may perpetuate <strong>social biases or stereotypes</strong>.</p>

    <h3>Mitigation Strategies</h3>
    <p>We undertook several design decisions to mitigate these harms:</p>
    <ul class="list-disc ml-6">
        <li><strong>Acceptable Use Policy (AUP):</strong> We clearly define and prohibit the creation of explicitly harmful, hateful, or misleading content.</li>
        <li><strong>The Feedback Loop:</strong> By actively soliciting feedback, we can swiftly identify and address *systemic* issues leading to biased or inappropriate outputs, allowing us to patch and fine-tune the model against these specific problems in future iterations.</li>
        <li><strong>Stylized Output Focus:</strong> While the MVP uses SDXL, our future lightweight model will be fine-tuned to encourage <strong>stylized, conceptual art</strong> over hyper-realistic deepfakes, making it inherently less prone to creating convincing misinformation.</li>
    </ul>

    <!-- 8. References -->
    <h2 class="text-2xl">8. References </h2>
    <ol class="list-decimal ml-6">
        <li><strong>Text-to-Image Generation Model:</strong>
            <p>Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. Available at: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" class="text-indigo-600 hover:text-indigo-800 underline" target="_blank">https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf</a></p>
        </li>
        <li><strong>Inference Platform:</strong>
            <p>Hugging Face Inference API Documentation, <em>The Hugging Face, Inc.</em> Available at: <a href="https://huggingface.co/" class="text-indigo-600 hover:text-indigo-800 underline" target="_blank">https://huggingface.co/</a></p>
        </li>
        <li><strong>Related Work & Inspirations:</strong>
            <p>OpenAI. (2023). DALL-E 3. [Link to DALL-E 3 blog post or related paper]. Available at: <a href="https://cookbook.openai.com/articles/what_is_new_with_dalle_3" class="text-indigo-600 hover:text-indigo-800 underline" target="_blank">https://cookbook.openai.com/articles/what_is_new_with_dalle_3</a></p>
        </li>
    </ol>

</div>

</body>
</html>
